{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "from torch import Tensor\n",
    "from typing import List\n",
    "\n",
    "\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s %(levelname)-8s %(message)s\",\n",
    "    level=logging.INFO,\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class args:\n",
    "    checkpoint = '/checkpoint/vincentqb/checkpoint/checkpoint-notebook-201007-1700.pth.tar'\n",
    "    dataset_train = 'train-clean-100'\n",
    "    dataset_valid = 'dev-clean'\n",
    "    dataset_test = 'test-clean'\n",
    "    batch_size = 128\n",
    "    start_epoch = 0\n",
    "    max_epoch = 1000\n",
    "    model_input_type = 'mfcc'\n",
    "    print_freq = 1\n",
    "    reduce_lr_valid = True\n",
    "    resume = False\n",
    "    workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we build an automatic speech recognition (ASR) training loop using pytorch and torchaudio. The goal is thus to recognize the text from an audio recording. The dataset used is [LibriSpeech](http://www.openslr.org/12/) and the model is [Wav2Letter](https://arxiv.org/abs/1609.03193). Both are available in torchaudio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset comprises of an audio recording of someone reading a target sentence, along with supplemental information. More precisely, a data point is `(waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id)`, and we need the waveform and the utterance from it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"url\": 'train-clean-100',\n",
    "    \"root\": '/datasets01/librispeech/',\n",
    "    \"folder_in_archive\": '062419',\n",
    "    \"download\": False\n",
    "}\n",
    "dataset_train = LIBRISPEECH(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id = dataset_train[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use only the first channel of the waveform, preprocess using the MFCC transform, and then normalize the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.transforms import MFCC\n",
    "\n",
    "\n",
    "def get_transforms(model_input_type, sample_rate=None, melkwargs=None):\n",
    "\n",
    "    if model_input_type == \"mfcc\":\n",
    "        n_mfcc = melkwargs[\"n_mels\"]\n",
    "        mfcc = MFCC(sample_rate=sample_rate, n_mfcc=n_mfcc, melkwargs=melkwargs)\n",
    "    \n",
    "    def transforms(tensor):\n",
    "        tensor = tensor[0, ...]  # downsample\n",
    "        if model_input_type == \"mfcc\":\n",
    "            tensor = mfcc(tensor)  # apply mfcc transform\n",
    "        tensor = (tensor - tensor.mean(-1, keepdim=True)) / tensor.std(-1, keepdim=True)  # normalize\n",
    "        return tensor\n",
    "\n",
    "    return transforms\n",
    "\n",
    "\n",
    "melkwargs = {\n",
    "    \"n_fft\": 400,\n",
    "    \"n_mels\": 13,\n",
    "    \"hop_length\": 160,\n",
    "}\n",
    "model_input_type = \"mfcc\"\n",
    "\n",
    "transforms = get_transforms(\n",
    "    model_input_type,\n",
    "    sample_rate,  # 16000 Hz for librispeech\n",
    "    melkwargs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.4363, -1.4952, -1.4389,  ..., -0.0213,  0.0154,  0.0940],\n",
       "        [ 0.1158, -0.1005, -0.1491,  ..., -1.1537, -1.1546, -1.1718],\n",
       "        [ 0.4979,  0.4646,  0.4829,  ..., -0.8638, -0.8657, -0.8790],\n",
       "        ...,\n",
       "        [ 0.3266,  0.2912, -0.3310,  ...,  0.3128,  0.1431,  0.5881],\n",
       "        [ 0.7309,  0.3005, -0.3413,  ...,  0.7735, -0.1685, -0.1231],\n",
       "        [-0.7672,  0.4788, -0.0693,  ...,  0.5826,  0.2591, -0.7708]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transforms(dataset_train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also introduces data augmentation techniques from SpecAugment during training. They are available in torchaudio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_augmentations(freq_mask, time_mask):\n",
    "    \n",
    "    if freq_mask:\n",
    "        fm = torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask)\n",
    "    if time_mask:\n",
    "        tm = torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
    "    \n",
    "    def transforms(tensor):\n",
    "        if freq_mask:\n",
    "            tensor = fm(tensor)\n",
    "        if time_mask:\n",
    "            tensor = tm(tensor)\n",
    "        return tensor\n",
    "\n",
    "    return transforms\n",
    "\n",
    "\n",
    "augmentations = get_augmentations(freq_mask=0, time_mask=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The utterance needs to be encoded into a tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections.abc import Iterable\n",
    "\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self, labels, char_blank, char_space):\n",
    "\n",
    "        self.char_space = char_space\n",
    "        self.char_blank = char_blank\n",
    "\n",
    "        enumerated = list(enumerate(labels))\n",
    "        flipped = [(sub[1], sub[0]) for sub in enumerated]\n",
    "\n",
    "        self._decode_map = dict(enumerated)\n",
    "        self._encode_map = dict(flipped)\n",
    "\n",
    "    def encode(self, listlike):\n",
    "        if not isinstance(listlike, str):\n",
    "            return [self.encode(i) for i in listlike]\n",
    "        return [self._encode_map[i] + self._encode_map[self.char_blank] for i in listlike]\n",
    "\n",
    "    def decode(self, tensor):\n",
    "        if len(tensor) > 0 and isinstance(tensor[0], Iterable):\n",
    "            return [self.decode(t) for t in tensor]\n",
    "\n",
    "        # not idempotent, since clean string\n",
    "        x = (self._decode_map[i] for i in tensor)\n",
    "        x = \"\".join(i for i, _ in itertools.groupby(x))\n",
    "        x = x.replace(self.char_blank, \"\")\n",
    "        # x = x.strip()\n",
    "        return x\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._encode_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text preprocessing\n",
    "\n",
    "char_blank = \"*\"\n",
    "char_space = \" \"\n",
    "char_apostrophe = \"'\"\n",
    "labels = char_blank + char_space + char_apostrophe + string.ascii_lowercase\n",
    "language_model = LanguageModel(labels, char_blank, char_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transforms\n",
    "\n",
    "melkwargs = {\n",
    "    \"n_fft\": 400,\n",
    "    \"n_mels\": 13,\n",
    "    \"hop_length\": 160,\n",
    "}\n",
    "sample_rate_original = 16000\n",
    "\n",
    "transforms = get_transforms(args.model_input_type, sample_rate_original, melkwargs)\n",
    "augmentations = get_augmentations(freq_mask=0, time_mask=0)\n",
    "\n",
    "# Text preprocessing\n",
    "\n",
    "char_blank = \"*\"\n",
    "char_space = \" \"\n",
    "char_apostrophe = \"'\"\n",
    "labels = char_blank + char_space + char_apostrophe + string.ascii_lowercase\n",
    "language_model = LanguageModel(labels, char_blank, char_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pprint\n",
    "import shutil\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "class Logger(defaultdict):\n",
    "    def __init__(self, name, print_freq=1, disable=False, filename=None, dataframe=None):\n",
    "        super().__init__(float)\n",
    "\n",
    "        self.disable = disable\n",
    "        self.print_freq = print_freq\n",
    "        self.filename = filename\n",
    "        self.dataframe = dataframe\n",
    "\n",
    "        self._name = \"name\"\n",
    "        self._time = \"elapsed time\"\n",
    "        self._iteration = \"iteration\"\n",
    "\n",
    "        self[self._name] = name\n",
    "        self[self._time] = time.monotonic()\n",
    "        self[self._iteration] = 0\n",
    "\n",
    "    def __repr__(self):\n",
    "        self[self._time] = time.monotonic() - self[self._time]\n",
    "        return dict.__repr__(self)\n",
    "    \n",
    "    def __str__(self):\n",
    "        return pprint.pformat(self)\n",
    "\n",
    "    def flush(self):\n",
    "        self[self._iteration] += 1\n",
    "        if not self[self._iteration] % self.print_freq:\n",
    "            if self.filename is not None:\n",
    "                self._append_to_file()\n",
    "            if self.dataframe is not None:\n",
    "                self._append_to_pandas()\n",
    "            if not self.disable:\n",
    "                print(self, flush=True)\n",
    "\n",
    "    def _append_to_file(self):\n",
    "        with open(self._filename, \"a\") as f:\n",
    "            f.write(self + \"\\n\")\n",
    "    \n",
    "    def _append_to_pandas(self):\n",
    "        self.dataframe = self.dataframe.append(self, ignore_index=True)\n",
    "\n",
    "\n",
    "def save_checkpoint(state, is_best, filename):\n",
    "    \"\"\"\n",
    "    Save the model to a temporary file first,\n",
    "    then copy it to filename, in case the signal interrupts\n",
    "    the torch.save() process.\n",
    "    \"\"\"\n",
    "\n",
    "    torch.save(state, filename)\n",
    "\n",
    "    if is_best:\n",
    "        shutil.copyfile(filename, \"best_\" + filename)\n",
    "\n",
    "    logging.warning(\"Checkpoint: saved\")\n",
    "\n",
    "\n",
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchaudio.datasets import LIBRISPEECH\n",
    "\n",
    "# Change backend for flac files\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "\n",
    "\n",
    "def pad_sequence(sequences, padding_value=0.0):\n",
    "    # type: (List[Tensor], float) -> Tensor\n",
    "    r\"\"\"Pad a list of variable length Tensors with ``padding_value``\n",
    "\n",
    "    ``pad_sequence`` stacks a list of Tensors along a new dimension,\n",
    "    and pads them to equal length. If the input is list of\n",
    "    sequences with size ``* x L`` then the output is and ``B x * x T``.\n",
    "\n",
    "    `B` is batch size. It is equal to the number of elements in ``sequences``.\n",
    "    `T` is length of the longest sequence.\n",
    "    `L` is length of the sequence.\n",
    "    `*` is any number of trailing dimensions, including none.\n",
    "\n",
    "    Example:\n",
    "        >>> from torch.nn.utils.rnn import pad_sequence\n",
    "        >>> a = torch.ones(300, 25)\n",
    "        >>> b = torch.ones(300, 22)\n",
    "        >>> c = torch.ones(300, 15)\n",
    "        >>> pad_sequence([a, b, c]).size()\n",
    "        torch.Size([300, 3, 25])\n",
    "\n",
    "    Note:\n",
    "        This function returns a Tensor of size ``B x * x T``\n",
    "        where `T` is the length of the longest sequence. This function assumes\n",
    "        trailing dimensions and type of all the Tensors in sequences are same.\n",
    "\n",
    "    Arguments:\n",
    "        sequences (list[Tensor]): list of variable length sequences.\n",
    "        padding_value (float, optional): value for padded elements. Default: 0.\n",
    "\n",
    "    Returns:\n",
    "        Tensor of size ``B x * x T``\n",
    "    \"\"\"\n",
    "\n",
    "    # assuming trailing dimensions and type of all the Tensors\n",
    "    # in sequences are same and fetching those from sequences[0]\n",
    "    max_size = sequences[0].size()\n",
    "    trailing_dims = max_size[:-1]\n",
    "    max_len = max([s.size(-1) for s in sequences])\n",
    "    out_dims = (len(sequences),) + trailing_dims + (max_len,)\n",
    "\n",
    "    out_tensor = sequences[0].new_full(out_dims, padding_value)\n",
    "    for i, tensor in enumerate(sequences):\n",
    "        length = tensor.size(-1)\n",
    "        # use index notation to prevent duplicate references to the tensor\n",
    "        out_tensor[i, ..., :length] = tensor\n",
    "\n",
    "    return out_tensor\n",
    "\n",
    "\n",
    "class ProcessedLIBRISPEECH(LIBRISPEECH):\n",
    "    def __init__(self, transforms, encode, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.transforms = transforms\n",
    "        self.encode = encode\n",
    "        self._cache = [None] * len(dataset)\n",
    "\n",
    "    def __getitem__(self, key):\n",
    "        # Cache results\n",
    "        if self._cache[key] is None:\n",
    "            item = super().__getitem__(key)\n",
    "            item = self.process_datapoint(item)\n",
    "            self._cache[key] = item\n",
    "        return self._cache[key]\n",
    "\n",
    "    def process_datapoint(self, item):\n",
    "        \"\"\"\n",
    "        Consume a LibriSpeech data point tuple:\n",
    "        (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id).\n",
    "        - transforms are applied to waveform. Output tensor shape (freq, time).\n",
    "        - target gets transformed into lower case, and encoded into a one dimensional long tensor.\n",
    "        \"\"\"\n",
    "        transformed = item[0]\n",
    "        target = item[2].lower()\n",
    "\n",
    "        transformed = self.transforms(transformed)\n",
    "\n",
    "        target = self.encode(target)\n",
    "        target = torch.tensor(target, dtype=torch.long,\n",
    "                              device=transformed.device)\n",
    "\n",
    "        return transformed, target\n",
    "\n",
    "\n",
    "def collate_factory(model_length_function, transforms=None):\n",
    "\n",
    "    if transforms is None:\n",
    "        transforms = torch.nn.Sequential()\n",
    "\n",
    "    def collate_fn(batch):\n",
    "\n",
    "        # apply transforms to waveforms\n",
    "        tensors = [transforms(b[0]) for b in batch]\n",
    "        tensors_lengths = torch.tensor(\n",
    "            [model_length_function(t) for t in tensors],\n",
    "            dtype=torch.long,\n",
    "            device=tensors[0].device,\n",
    "        )\n",
    "        tensors = pad_sequence(tensors)\n",
    "\n",
    "        # extract target utterance\n",
    "        targets = [b[1] for b in batch]\n",
    "        target_lengths = torch.tensor(\n",
    "            [target.shape[0] for target in targets],\n",
    "            dtype=torch.long,\n",
    "            device=tensors.device,\n",
    "        )\n",
    "        targets = pad_sequence(targets)\n",
    "\n",
    "        return tensors, targets, tensors_lengths, target_lengths\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import topk\n",
    "\n",
    "\n",
    "class GreedyDecoder:\n",
    "    def __call__(self, outputs):\n",
    "        \"\"\"Greedy Decoder. Returns highest probability of class labels for each timestep\n",
    "\n",
    "        Args:\n",
    "            outputs (torch.Tensor): shape (input length, batch size, number of classes (including blank))\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: class labels per time step.\n",
    "        \"\"\"\n",
    "        _, indices = topk(outputs, k=1, dim=-1)\n",
    "        return indices[..., 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Union\n",
    "\n",
    "\n",
    "def levenshtein_distance(r: Union[str, List[str]], h: Union[str, List[str]]) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the Levenshtein distance between two lists or strings.\n",
    "\n",
    "    The function computes an edit distance allowing deletion, insertion and substitution.\n",
    "    The result is an integer. Users may want to normalize by the length of the reference.\n",
    "\n",
    "    Args:\n",
    "        r (str or List[str]): the reference list or string to compare.\n",
    "        h (str or List[str]): the hypothesis, the predicted list or string, to compare.\n",
    "    Returns:\n",
    "        int: The distance between the reference and the hypothesis.\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialisation\n",
    "    dold = list(range(len(h) + 1))\n",
    "    dnew = list(0 for _ in range(len(h) + 1))\n",
    "\n",
    "    # Computation\n",
    "    for i in range(1, len(r) + 1):\n",
    "        dnew[0] = i\n",
    "        for j in range(1, len(h) + 1):\n",
    "            if r[i - 1] == h[j - 1]:\n",
    "                dnew[j] = dold[j - 1]\n",
    "            else:\n",
    "                substitution = dold[j - 1] + 1\n",
    "                insertion = dnew[j - 1] + 1\n",
    "                deletion = dold[j] + 1\n",
    "                dnew[j] = min(substitution, insertion, deletion)\n",
    "\n",
    "        dnew, dold = dold, dnew\n",
    "\n",
    "    return dold[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import string\n",
    "\n",
    "\n",
    "def get_augmentations(freq_mask, time_mask):\n",
    "    \n",
    "    if freq_mask:\n",
    "        fm = torchaudio.transforms.FrequencyMasking(freq_mask_param=freq_mask)\n",
    "    if time_mask:\n",
    "        tm = torchaudio.transforms.TimeMasking(time_mask_param=time_mask)\n",
    "    \n",
    "    def transforms(tensor):\n",
    "        if freq_mask:\n",
    "            tensor = fm(tensor)\n",
    "        if time_mask:\n",
    "            tensor = tm(tensor)\n",
    "        return tensor\n",
    "\n",
    "    return transforms\n",
    "\n",
    "\n",
    "def model_length_function_constructor(model_input_type):\n",
    "    if model_input_type == \"waveform\":\n",
    "        return lambda tensor: int(tensor.shape[-1]) // 160 // 2 + 1\n",
    "    elif model_input_type == \"mfcc\":\n",
    "        return lambda tensor: int(tensor.shape[-1]) // 2 + 1\n",
    "    raise NotImplementedError(\n",
    "        f\"Selected model input type {model_input_type} not supported\"\n",
    "    )\n",
    "\n",
    "\n",
    "def record_losses(outputs, targets, decoder, language_model, loss_value, metric):\n",
    "\n",
    "    # outputs: input length, batch size, number of classes (including blank)\n",
    "    metric[\"batch size\"] = outputs.shape[1]\n",
    "    metric[\"cumulative batch size\"] += metric[\"batch size\"]\n",
    "\n",
    "    # Record loss\n",
    "\n",
    "    metric[\"cumulative loss\"] += loss_value\n",
    "    metric[\"epoch loss\"] = metric[\"cumulative loss\"] / metric[\"cumulative batch size\"]\n",
    "    metric[\"batch loss\"] = loss_value / metric[\"batch size\"]\n",
    "\n",
    "    # Decode output\n",
    "\n",
    "    output = outputs.transpose(0, 1).to(\"cpu\")\n",
    "    output = decoder(output)\n",
    "\n",
    "    # Compute CER\n",
    "\n",
    "    output = language_model.decode(output.tolist())\n",
    "    target = language_model.decode(targets.tolist())\n",
    "\n",
    "    cers = [levenshtein_distance(t, o) for t, o in zip(target, output)]\n",
    "    cers = sum(cers)\n",
    "    n = sum(len(t) for t in target)\n",
    "\n",
    "    metric[\"total chars\"] += n\n",
    "    metric[\"cumulative char errors\"] += cers\n",
    "    metric[\"batch cer\"] = cers / n\n",
    "    metric[\"epoch cer\"] = metric[\"cumulative char errors\"] / metric[\"total chars\"]\n",
    "\n",
    "    # Print a few output/target pairs\n",
    "\n",
    "    print_length = 20\n",
    "    for i in range(2):\n",
    "        # Print a few examples\n",
    "        output_print = output[i].ljust(print_length)[:print_length]\n",
    "        target_print = target[i].ljust(print_length)[:print_length]\n",
    "        logging.info(\"Target: %s    | Output: %s\", target_print, output_print)\n",
    "\n",
    "    # Compute WER\n",
    "\n",
    "    output = [o.split(language_model.char_space) for o in output]\n",
    "    target = [t.split(language_model.char_space) for t in target]\n",
    "\n",
    "    wers = [levenshtein_distance(t, o) for t, o in zip(target, output)]\n",
    "    wers = sum(wers)\n",
    "    n = sum(len(t) for t in target)\n",
    "\n",
    "    metric[\"total words\"] += n\n",
    "    metric[\"cumulative word errors\"] += wers\n",
    "    metric[\"batch wer\"] = wers / n\n",
    "    metric[\"epoch wer\"] = metric[\"cumulative word errors\"] / metric[\"total words\"]\n",
    "\n",
    "    return metric[\"epoch loss\"]\n",
    "\n",
    "\n",
    "def train_one_epoch(\n",
    "    model,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    data_loader,\n",
    "    decoder,\n",
    "    language_model,\n",
    "    device,\n",
    "    metric,\n",
    "):\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for inputs, targets, tensors_lengths, target_lengths in bg_iterator(\n",
    "        data_loader, maxsize=2\n",
    "    ):\n",
    "\n",
    "        inputs = inputs.to(device, non_blocking=True)\n",
    "        targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "        # keep batch first for data parallel\n",
    "        outputs = model(inputs).transpose(-1, -2).transpose(0, 1)\n",
    "\n",
    "        # CTC\n",
    "        # outputs: input length, batch size, number of classes (including blank)\n",
    "        # targets: batch size, max target length\n",
    "        # input_lengths: batch size\n",
    "        # target_lengths: batch size\n",
    "\n",
    "        loss = criterion(outputs, targets, tensors_lengths, target_lengths)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        avg_loss = record_losses(\n",
    "            outputs, targets, decoder, language_model, loss.item(), metric\n",
    "        )\n",
    "\n",
    "        metric[\"lr\"] = optimizer.param_groups[0][\"lr\"]\n",
    "        metric[\"channel size\"] = inputs.shape[1]\n",
    "        metric[\"time size\"] = inputs.shape[-1]\n",
    "        metric.flush()\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model,\n",
    "    criterion,\n",
    "    data_loader,\n",
    "    decoder,\n",
    "    language_model,\n",
    "    device,\n",
    "    epoch,\n",
    "    metric,\n",
    "):\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        for inputs, targets, tensors_lengths, target_lengths in bg_iterator(\n",
    "            data_loader, maxsize=2\n",
    "        ):\n",
    "\n",
    "            inputs = inputs.to(device, non_blocking=True)\n",
    "            targets = targets.to(device, non_blocking=True)\n",
    "\n",
    "            # keep batch first for data parallel\n",
    "            outputs = model(inputs).transpose(-1, -2).transpose(0, 1)\n",
    "\n",
    "            # CTC\n",
    "            # outputs: input length, batch size, number of classes (including blank)\n",
    "            # targets: batch size, max target length\n",
    "            # input_lengths: batch size\n",
    "            # target_lengths: batch size\n",
    "\n",
    "            loss = criterion(outputs, targets, tensors_lengths, target_lengths)\n",
    "\n",
    "            avg_loss = record_losses(\n",
    "                outputs, targets, decoder, language_model, loss.item(), metric\n",
    "            )\n",
    "\n",
    "        metric.flush()\n",
    "\n",
    "        return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import Adadelta\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import DataLoader\n",
    "from torchaudio.datasets.utils import bg_iterator\n",
    "from torchaudio.models.wav2letter import Wav2Letter\n",
    "\n",
    "\n",
    "logging.info(\"Start\")\n",
    "\n",
    "# Change backend for flac files\n",
    "torchaudio.set_audio_backend(\"soundfile\")\n",
    "\n",
    "\n",
    "# Dataset loader\n",
    "\n",
    "kwargs = {\n",
    "    \"transforms\": transforms,\n",
    "    \"encode\", language_model.encode,\n",
    "    \"root\": args.dataset_root,\n",
    "    \"folder_in_archive\": args.folder_in_archive,\n",
    "    \"download\": False\n",
    "}\n",
    "dataset_train = ProcessedLIBRISPEECH(args.dataset_train, **kwargs)\n",
    "dataset_valid = ProcessedLIBRISPEECH(args.dataset_valid, **kwargs)\n",
    "if args.dataset_test:\n",
    "    dataset_test = ProcessedLIBRISPEECH(args.dataset_test,**kwargs)\n",
    "\n",
    "model_length_function = model_length_function_constructor(args.model_input_type)\n",
    "collate_fn_train = collate_factory(model_length_function, augmentations)\n",
    "collate_fn_valid = collate_factory(model_length_function)\n",
    "\n",
    "loader_train = DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collate_fn_train,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "loader_valid = DataLoader(\n",
    "    dataset_valid,\n",
    "    batch_size=args.batch_size,\n",
    "    collate_fn=collate_fn_valid,\n",
    "    num_workers=args.workers,\n",
    "    pin_memory=True,\n",
    "    shuffle=False,\n",
    "    drop_last=False,\n",
    ")\n",
    "if args.dataset_test:\n",
    "    loader_test = DataLoader(\n",
    "        dataset_test,\n",
    "        batch_size=args.batch_size,\n",
    "        collate_fn=collate_fn_valid,\n",
    "        num_workers=args.workers,\n",
    "        pin_memory=True,\n",
    "        shuffle=False,\n",
    "        drop_last=False,\n",
    "    )\n",
    "\n",
    "# Decoder\n",
    "\n",
    "decoder = GreedyDecoder()\n",
    "\n",
    "# Model\n",
    "\n",
    "model = Wav2Letter(\n",
    "    num_classes=len(language_model),\n",
    "    input_type=args.model_input_type,\n",
    "    num_features=melkwargs[\"n_mels\"],\n",
    ")\n",
    "\n",
    "devices = [\"cuda\" if torch.cuda.is_available() else \"cpu\"]\n",
    "model = model.to(devices[0], non_blocking=True)\n",
    "model = torch.nn.DataParallel(model)\n",
    "\n",
    "n = count_parameters(model)\n",
    "logging.info(\"Number of parameters: %s\", n)\n",
    "\n",
    "# Optimizer\n",
    "\n",
    "optimizer = Adadelta(\n",
    "    model.parameters(),\n",
    "    lr=0.6,\n",
    "    weight_decay=1e-05,\n",
    "    eps=1e-08,\n",
    "    rho=0.95,\n",
    ")\n",
    "scheduler = ReduceLROnPlateau(optimizer, patience=10, threshold=1e-3)\n",
    "\n",
    "# Loss\n",
    "\n",
    "encoded_char_blank = language_model.encode(char_blank)[0]\n",
    "criterion = torch.nn.CTCLoss(blank=encoded_char_blank, zero_infinity=False, reduction=\"sum\")\n",
    "best_loss = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup checkpoint\n",
    "\n",
    "checkpoint_exists = os.path.isfile(args.checkpoint)\n",
    "\n",
    "if args.checkpoint and checkpoint_exists and args.resume:\n",
    "    logging.info(\"Checkpoint loading %s\", args.checkpoint)\n",
    "    checkpoint = torch.load(args.checkpoint)\n",
    "\n",
    "    args.start_epoch = checkpoint[\"epoch\"]\n",
    "    best_loss = checkpoint[\"best_loss\"]\n",
    "\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    scheduler.load_state_dict(checkpoint[\"scheduler\"])\n",
    "\n",
    "    logging.info(\n",
    "        \"Checkpoint loaded '%s' at epoch %s\", args.checkpoint, checkpoint[\"epoch\"]\n",
    "    )\n",
    "elif args.checkpoint and checkpoint_exists:\n",
    "    raise RuntimeError(\n",
    "        \"Checkpoint already exists. Set resume to True, or manually delete existing file.\"\n",
    "    )\n",
    "elif args.checkpoint and args.resume:\n",
    "    raise RuntimeError(\"Checkpoint not found\")\n",
    "elif args.checkpoint:\n",
    "    save_checkpoint(\n",
    "        {\n",
    "            \"epoch\": args.start_epoch,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"best_loss\": best_loss,\n",
    "            \"optimizer\": optimizer.state_dict(),\n",
    "            \"scheduler\": scheduler.state_dict(),\n",
    "        },\n",
    "        False,\n",
    "        args.checkpoint,\n",
    "    )\n",
    "elif not args.checkpoint and args.resume:\n",
    "    raise RuntimeError(\"Checkpoint not provided. Use checkpoint to specify.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataframe_log = pd.DataFrame()\n",
    "\n",
    "for epoch in range(args.start_epoch, args.max_epoch):\n",
    "\n",
    "    logging.info(\"Epoch: %s\", epoch)\n",
    "\n",
    "    metric = Logger(\"train\", dataframe=dataframe_log)\n",
    "    metric[\"epoch\"] = epoch\n",
    "\n",
    "    train_one_epoch(\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        loader_train,\n",
    "        decoder,\n",
    "        language_model,\n",
    "        devices[0],\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "    metric = Logger(\"validation\", dataframe=dataframe_log)\n",
    "    metric[\"epoch\"] = epoch\n",
    "\n",
    "    loss = evaluate(\n",
    "        model,\n",
    "        criterion,\n",
    "        loader_valid,\n",
    "        decoder,\n",
    "        language_model,\n",
    "        devices[0],\n",
    "        epoch,\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "    scheduler.step(loss)\n",
    "\n",
    "    is_best = loss < best_loss\n",
    "    best_loss = min(loss, best_loss)\n",
    "    if args.checkpoint:\n",
    "        save_checkpoint(\n",
    "            {\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"best_loss\": best_loss,\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "            },\n",
    "            is_best,\n",
    "            args.checkpoint,\n",
    "        )\n",
    "\n",
    "    metric = Logger(\"test\", dataframe=dataframe_log)\n",
    "    metric[\"epoch\"] = epoch\n",
    "\n",
    "    evaluate(\n",
    "        model,\n",
    "        criterion,\n",
    "        loader_test,\n",
    "        decoder,\n",
    "        language_model,\n",
    "        devices[0],\n",
    "        epoch,\n",
    "        metric,\n",
    "    )\n",
    "\n",
    "logging.info(\"End\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
